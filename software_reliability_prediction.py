# -*- coding: utf-8 -*-
"""SOFTWARE RELIABILITY PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q99G3cNAvmK7tKlTHsxGs4bBG1V8AjGe
"""

from google.colab import files
import pandas as pd
from scipy.io import arff

uploaded = files.upload()

data = arff.loadarff(next(iter(uploaded)))
df = pd.DataFrame(data[0])

df.head()

df.shape

df.to_csv('promise_dataset.csv')

"""Target column is 'DL'"""

df['DL'].value_counts()

df['DL'].unique()

# CONVERT THE TARGET COLUMN FROM OBJECT TO INTEGER DATA TYPE
import pandas as pd
import numpy as np

# Convert the 'DL' column to string type
df['DL'] = df['DL'].astype(str)

# Create a new column to store the extracted DL values using a more flexible pattern
df['DL_extracted'] = np.where(df['DL'].str.contains('_TRUE', case=False, na=False), 1,
                             np.where(df['DL'].str.contains('_FALSE', case=False, na=False) | df['DL'].str.contains('false', case=False, na=False), 0, -1))
                             # Added a condition to also check for 'false'

# Verify the transformation
print(df['DL_extracted'].value_counts(), df['DL_extracted'].dtype)

df.head()

df['DL_extracted'].value_counts()

# now we have two target columns, so delete DL
# Drop the 'DL' column
df = df.drop('DL', axis=1)

# Display the updated DataFrame (optional)
df.head()

# Rename the 'DL_extracted' column to 'DL'
df = df.rename(columns={'DL_extracted': 'DL'})

# Display the updated DataFrame (optional)
df.head()

"""CHECK FOR NULL VALUES"""

df.isnull().sum().sum()

# so there are no null values in the data set

"""check for correlation between columns"""

import seaborn as sns
import matplotlib.pyplot as plt

# Step 1: Compute the correlation matrix
correlation_matrix = df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, cmap="coolwarm", annot=False, fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Matrix")
plt.show()

# here we can see the columns with high correlation instead of all columns
# we use threshold level to show columns above correlation 0.85

mask = np.abs(correlation_matrix) > 0.90  # Using absolute value to consider both positive and negative correlations

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix,
            cmap="coolwarm",
            annot=False,
            fmt=".2f",
            linewidths=0.5,
            mask=mask)  # Apply the mask to hide low correlation values
plt.title("Feature Correlation Matrix (Highlighting > 0.85)")

"""now we will split the data into features(x) and target(y)"""

# Step 1: Split dataset into X (features) and y (target variable)

# Define the target variable (y) and features (X)
X = df.drop(columns=['DL'])  # All columns except 'DL' are features
y = df['DL']  # Target column

# Verify the shapes of X and y
X.shape, y.shape

"""now we will split the x and y into training(80%) and testing(20%)"""

from sklearn.model_selection import train_test_split

# Step 2: Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

# Verify the shapes of the split data
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""we will build Machine Learning Models

1. LOGISTIC REGRESSION
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Step 3: Train Logistic Regression Model
log_reg = LogisticRegression(random_state=0)
log_reg.fit(X_train, y_train)

# Step 4: Make Predictions
y_pred_LR = log_reg.predict(X_test)

# Step 5: Evaluate Model Performance
accuracy = accuracy_score(y_test, y_pred_LR)
precision = precision_score(y_test, y_pred_LR)
recall = recall_score(y_test, y_pred_LR)
f1 = f1_score(y_test, y_pred_LR)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

"""2. DECISION TREE MODEL"""

from sklearn.tree import DecisionTreeClassifier

# Step 1: Train Decision Tree Model
dt_model = DecisionTreeClassifier(random_state=0)
dt_model.fit(X_train, y_train)

# Step 2: Make Predictions
y_pred_DT = dt_model.predict(X_test)

# Step 3: Evaluate Model Performance
accuracy_dt = accuracy_score(y_test, y_pred_DT)
precision_dt = precision_score(y_test, y_pred_DT)
recall_dt = recall_score(y_test, y_pred_DT)
f1_dt = f1_score(y_test, y_pred_DT)

print("Accuracy:", accuracy_dt)
print("Precision:", precision_dt)
print("Recall:", recall_dt)
print("F1 Score:", f1_dt)

"""3. RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

# Step 1: Train Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=0)
rf_model.fit(X_train, y_train)

# Step 2: Make Predictions
y_pred_RF = rf_model.predict(X_test)

# Step 3: Evaluate Model Performance
accuracy_rf = accuracy_score(y_test, y_pred_RF)
precision_rf = precision_score(y_test, y_pred_RF)
recall_rf = recall_score(y_test, y_pred_RF)
f1_rf = f1_score(y_test, y_pred_RF)

print("Accuracy:", accuracy_rf)
print("Precision:", precision_rf)
print("Recall:", recall_rf)
print("F1 Score:", f1_rf)

"""4. XGBOOST"""

from xgboost import XGBClassifier

# Step 1: Train XGBoost Model
xgb_model = XGBClassifier(eval_metric="logloss", random_state=0)
xgb_model.fit(X_train, y_train)

# Step 2: Make Predictions
y_pred_XGB = xgb_model.predict(X_test)

# Step 3: Evaluate Model Performance
accuracy_xgb = accuracy_score(y_test, y_pred_XGB)
precision_xgb = precision_score(y_test, y_pred_XGB)
recall_xgb = recall_score(y_test, y_pred_XGB)
f1_xgb = f1_score(y_test, y_pred_XGB)

print("Accuracy:", accuracy_xgb)
print("Precision:", precision_xgb)
print("Recall:", recall_xgb)
print("F1 Score:", f1_xgb)

"""5. ARTIFICIAL NEURAL NETWORK"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Step 1: Define the ANN model
ann_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden Layer 1
    Dense(32, activation='relu'),  # Hidden Layer 2
    Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)
])

# Step 2: Compile the model
ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 3: Train the model
history = ann_model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test), verbose=1)

# Step 4: Make Predictions
y_pred_ANN = (ann_model.predict(X_test) > 0.5).astype(int).flatten()

# Step 5: Evaluate Model Performance
accuracy_ann = accuracy_score(y_test, y_pred_ANN)
precision_ann = precision_score(y_test, y_pred_ANN)
recall_ann = recall_score(y_test, y_pred_ANN)
f1_ann = f1_score(y_test, y_pred_ANN)

print("Accuracy:", accuracy_ann)
print("Precision:", precision_ann)
print("Recall:", recall_ann)
print("F1 Score:", f1_ann)

"""6. HYBID FUZZY LOGIC-ANN MODEL"""

# FOR FUZZY LOGIC TO BE IMPLEMENTED WE NEED TO SELECT HIGHLY CORRELATED FEATURES
# WE WILL IDENTIFY TOP 5 FEATURES WITH CORRELATION MATRIX

# Step 1: Compute the correlation matrix with respect to the target variable 'DL'
correlation_matrix = df.corr()['DL'].abs().sort_values(ascending=False)

# Step 2: Select the top 5 highly correlated features (excluding 'DL' itself)
top_fuzzy_features = correlation_matrix.index[1:6]  # Skipping 'DL' which is at index 0

# Display selected features for fuzzy transformation
top_fuzzy_features

# so now we know the important features
# now we should apply membership functions on them

"""now combine the fuzzy defect likelihood as training data to build hybrid model"""

# Step 1: Compute percentiles for dynamic fuzzy membership ranges
low_threshold = df[['maxNUM_UNIQUE_OPERANDS', 'maxNUM_UNIQUE_OPERATORS',
                    'COUPLING_BETWEEN_OBJECTS', 'maxHALSTEAD_DIFFICULTY', 'maxNUM_OPERANDS']].quantile(0.33)

medium_threshold = df[['maxNUM_UNIQUE_OPERANDS', 'maxNUM_UNIQUE_OPERATORS',
                       'COUPLING_BETWEEN_OBJECTS', 'maxHALSTEAD_DIFFICULTY', 'maxNUM_OPERANDS']].quantile(0.66)

# Step 2: Define new fuzzy membership function (Low, Medium, High) using percentile-based classification
def fuzzy_membership(value, feature):
    if value <= low_threshold[feature]:
        return 0.2  # Low
    elif value <= medium_threshold[feature]:
        return 0.5  # Medium
    else:
        return 0.8  # High

# Step 3: Apply refined fuzzy transformation to compute new fuzzy_defect_likelihood scores
df['refined_fuzzy_defect_likelihood'] = df.apply(
    lambda row: (fuzzy_membership(row['maxNUM_UNIQUE_OPERANDS'], 'maxNUM_UNIQUE_OPERANDS') +
                 fuzzy_membership(row['maxNUM_UNIQUE_OPERATORS'], 'maxNUM_UNIQUE_OPERATORS') +
                 fuzzy_membership(row['COUPLING_BETWEEN_OBJECTS'], 'COUPLING_BETWEEN_OBJECTS') +
                 fuzzy_membership(row['maxHALSTEAD_DIFFICULTY'], 'maxHALSTEAD_DIFFICULTY') +
                 fuzzy_membership(row['maxNUM_OPERANDS'], 'maxNUM_OPERANDS')) / 5, axis=1)

#



# Step 1: Add refined fuzzy feature to training data
X_train['refined_fuzzy_defect_likelihood'] = df.loc[X_train.index, 'refined_fuzzy_defect_likelihood']
X_test['refined_fuzzy_defect_likelihood'] = df.loc[X_test.index, 'refined_fuzzy_defect_likelihood']

# Step 2: Define the updated ANN model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

ann_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden Layer 1
    Dropout(0.2),  # Adding dropout for better generalization
    Dense(32, activation='relu'),  # Hidden Layer 2
    Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)
])

# Step 3: Compile the model
ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 4: Train the model with early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = ann_model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test),
                        verbose=1, callbacks=[early_stopping])

# Step 5: Make Predictions
y_pred_ANN_refined = (ann_model.predict(X_test) > 0.5).astype(int).flatten()

# Step 6: Evaluate Model Performance
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy_ann_refined = accuracy_score(y_test, y_pred_ANN_refined)
precision_ann_refined = precision_score(y_test, y_pred_ANN_refined)
recall_ann_refined = recall_score(y_test, y_pred_ANN_refined)
f1_ann_refined = f1_score(y_test, y_pred_ANN_refined)

print("Accuracy:", accuracy_ann_refined)
print("Precision:", precision_ann_refined)
print("Recall:", recall_ann_refined)
print("F1 Score:", f1_ann_refined)

# comparison of model performance
import pandas as pd

# Assuming you have the accuracy, precision, recall, and F1-score for each model stored in variables
# like accuracy, precision, recall, f1, accuracy_dt, precision_dt, etc.

# Create a dictionary to store the results
results_data = {
    "Model": ["Logistic Regression", "Decision Tree", "Random Forest", "XGBoost", "ANN", "Hybrid ANN-Fuzzy"],
    "Accuracy": [accuracy, accuracy_dt, accuracy_rf, accuracy_xgb, accuracy_ann, accuracy_ann_refined],
    "Precision": [precision, precision_dt, precision_rf, precision_xgb, precision_ann, precision_ann_refined],
    "Recall": [recall, recall_dt, recall_rf, recall_xgb, recall_ann, recall_ann_refined],
    "F1-Score": [f1, f1_dt, f1_rf, f1_xgb, f1_ann, f1_ann_refined]
}

# Create a Pandas DataFrame from the dictionary
results_df = pd.DataFrame(results_data)

# Display the DataFrame as a table
display(results_df)



import tensorflow as tf
ann_model.save('model.h5')  # Save model

pip install flask

from flask import Flask, request, jsonify
import pickle
import numpy as np
import tensorflow as tf # Import TensorFlow at the beginning

# Load model
with open('model.h5', 'rb') as file:
    # Load model using tf.keras.models.load_model()
    model = tf.keras.models.load_model('model.h5')
    # Re-compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


app = Flask(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json  # Get JSON input
    features = np.array(data['features']).reshape(1, -1)  # Convert to array
    prediction = model.predict(features)  # Predict
    return jsonify({'prediction': prediction.tolist()})  # Return response

if __name__ == '__main__':
    app.run(debug=True)