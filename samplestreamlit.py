# -*- coding: utf-8 -*-
"""samplestreamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sN61vhDSbHT8JhoTocrJn5boF4LW55JW
"""
import streamlit as st
import pandas as pd

st.title("üîç Software Reliability Prediction")

uploaded_file = st.file_uploader("Upload your dataset (CSV)", type=['csv'])

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)  # Read uploaded file
    st.write("Preview of Data:")
    st.dataframe(df.head())  # Show first 5 rows

import pandas as pd
df = pd.read_csv('promise_dataset.csv')

df.head()

df.shape

df.drop('Unnamed: 0',axis=1,inplace=True)

# CONVERT THE TARGET COLUMN FROM OBJECT TO INTEGER DATA TYPE
import pandas as pd
import numpy as np

# Convert the 'DL' column to string type
df['DL'] = df['DL'].astype(str)

# Create a new column to store the extracted DL values using a more flexible pattern
df['DL_extracted'] = np.where(df['DL'].str.contains('_TRUE', case=False, na=False), 1,
                             np.where(df['DL'].str.contains('_FALSE', case=False, na=False) | df['DL'].str.contains('false', case=False, na=False), 0, -1))
                             # Added a condition to also check for 'false'

# Verify the transformation
print(df['DL_extracted'].value_counts(), df['DL_extracted'].dtype)

# now we have two target columns, so delete DL
# Drop the 'DL' column
df = df.drop('DL', axis=1)

# Display the updated DataFrame (optional)
df.head()

# Rename the 'DL_extracted' column to 'DL'
df = df.rename(columns={'DL_extracted': 'DL'})

# Display the updated DataFrame (optional)
df.head()

# Step 1: Split dataset into X (features) and y (target variable)

# Define the target variable (y) and features (X)
X = df.drop(columns=['DL'])  # All columns except 'DL' are features
y = df['DL']  # Target column

# Verify the shapes of X and y
X.shape, y.shape

from sklearn.model_selection import train_test_split

# Step 2: Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

# Verify the shapes of the split data
X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Step 3: Train Logistic Regression Model
log_reg = LogisticRegression(random_state=0)
log_reg.fit(X_train, y_train)

# Step 4: Make Predictions
y_pred_LR = log_reg.predict(X_test)

# Step 5: Evaluate Model Performance
accuracy = accuracy_score(y_test, y_pred_LR)
precision = precision_score(y_test, y_pred_LR)
recall = recall_score(y_test, y_pred_LR)
f1 = f1_score(y_test, y_pred_LR)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Step 1: Compute percentiles for dynamic fuzzy membership ranges
low_threshold = df[['maxNUM_UNIQUE_OPERANDS', 'maxNUM_UNIQUE_OPERATORS',
                    'COUPLING_BETWEEN_OBJECTS', 'maxHALSTEAD_DIFFICULTY', 'maxNUM_OPERANDS']].quantile(0.33)

medium_threshold = df[['maxNUM_UNIQUE_OPERANDS', 'maxNUM_UNIQUE_OPERATORS',
                       'COUPLING_BETWEEN_OBJECTS', 'maxHALSTEAD_DIFFICULTY', 'maxNUM_OPERANDS']].quantile(0.66)

# Step 2: Define new fuzzy membership function (Low, Medium, High) using percentile-based classification
def fuzzy_membership(value, feature):
    if value <= low_threshold[feature]:
        return 0.2  # Low
    elif value <= medium_threshold[feature]:
        return 0.5  # Medium
    else:
        return 0.8  # High

# Step 3: Apply refined fuzzy transformation to compute new fuzzy_defect_likelihood scores
df['refined_fuzzy_defect_likelihood'] = df.apply(
    lambda row: (fuzzy_membership(row['maxNUM_UNIQUE_OPERANDS'], 'maxNUM_UNIQUE_OPERANDS') +
                 fuzzy_membership(row['maxNUM_UNIQUE_OPERATORS'], 'maxNUM_UNIQUE_OPERATORS') +
                 fuzzy_membership(row['COUPLING_BETWEEN_OBJECTS'], 'COUPLING_BETWEEN_OBJECTS') +
                 fuzzy_membership(row['maxHALSTEAD_DIFFICULTY'], 'maxHALSTEAD_DIFFICULTY') +
                 fuzzy_membership(row['maxNUM_OPERANDS'], 'maxNUM_OPERANDS')) / 5, axis=1)

#

# Step 1: Add refined fuzzy feature to training data
X_train['refined_fuzzy_defect_likelihood'] = df.loc[X_train.index, 'refined_fuzzy_defect_likelihood']
X_test['refined_fuzzy_defect_likelihood'] = df.loc[X_test.index, 'refined_fuzzy_defect_likelihood']

# Step 2: Define the updated ANN model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

ann_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden Layer 1
    Dropout(0.2),  # Adding dropout for better generalization
    Dense(32, activation='relu'),  # Hidden Layer 2
    Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)
])

# Step 3: Compile the model
ann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Step 4: Train the model with early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = ann_model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test),
                        verbose=1, callbacks=[early_stopping])

# Step 5: Make Predictions
y_pred_ANN_refined = (ann_model.predict(X_test) > 0.5).astype(int).flatten()

# Step 6: Evaluate Model Performance
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy_ann_refined = accuracy_score(y_test, y_pred_ANN_refined)
precision_ann_refined = precision_score(y_test, y_pred_ANN_refined)
recall_ann_refined = recall_score(y_test, y_pred_ANN_refined)
f1_ann_refined = f1_score(y_test, y_pred_ANN_refined)

print("Accuracy:", accuracy_ann_refined)
print("Precision:", precision_ann_refined)
print("Recall:", recall_ann_refined)
print("F1 Score:", f1_ann_refined)

ann_model.save('model2.h5')

